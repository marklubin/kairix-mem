Local LLM Choices (March 2025) and Hardware Considerations

Top Locally-Run LLMs (March 2025) Compatible with Ollama/LiteLLM

Several state-of-the-art open-source LLMs can run locally and meet your requirements for system prompt adherence, tool use (function calling), and large context windows:
	•	EverythingLM 13B (Llama-2 based) – A fine-tuned Llama-2 chat model with a 16k token context window ￼. It is instruction-tuned to follow system prompts (e.g. role and behavior directives) and can handle long conversations or documents. EverythingLM is available through Ollama (quantized ~9GB) ￼ and works with LiteLLM. Being Llama-2-based, it inherits good general knowledge and chat alignment, making it reliable in following a system role.
	•	Qwen-14B-Chat (Alibaba) – A 14B parameter chat model known for exceptional tool-use and function calling capabilities. Qwen-Chat was explicitly optimized for function calling (e.g. deciding when to invoke tools/functions) ￼, achieving ~97% tool selection accuracy in benchmarks (approaching GPT-4) ￼. It adheres well to system instructions and supports complex multi-turn prompts (Alibaba highlights its ability for role-playing and system-driven behavior customization ￼). Qwen-14B has an 8k context window natively ￼ – slightly below your 16k preference, but its strong reasoning and tool-use may outweigh that. (Larger-context variants of Qwen are emerging – e.g. Qwen-1.8B had 32k ￼, and the Qwen 2.5 series introduces 128k contexts – though those bigger versions may not be fully open-source yet.) Qwen-14B is Apache-2.0 licensed and can be run locally via HuggingFace transformers or as a quantized model in Ollama (community conversions exist).
	•	Mistral 7B Instruct v0.2 – A smaller (7B) model that offers impressively long context (32k tokens) ￼. Mistral-7B is fine-tuned for instructions (chat) and was one of the first open models to natively support 32k context windows, enabling very long conversations or documents. It can follow system prompts reasonably well (given its instruct tuning) and has shown strong performance relative to its size. At 7B it will run efficiently on your hardware (even in higher precision) and leaves GPU headroom. This model is compatible with Ollama/LiteLLM (it’s open source ￼ and available in many model libraries). While its smaller size means it won’t match a 13B+ model in raw capability, Mistral is very efficient and can be a good choice when memory is limited or maximum context length is a priority.
	•	Cohere Command (Command-RT) – Cohere’s instruct model (7B, 35B, up to 104B sizes) is notable for having been fine-tuned for tool use and long-form chat. The 7B and 35B “Command” models support up to 128k context ￼ and are designed for conversational AI with function/tool usage in mind (Cohere mentions these use cases). However, they are released under a non-commercial license ￼. If your use is non-commercial or you have access, the 7B Command model could be run locally (and is listed in the Ollama library). It’s geared toward following instructions and even has some support for tools (the Ollama team has integrated “Command-R” with function calling in mind ￼). For a fully open solution, the above models (Llama-based, Qwen, Mistral) are safer bets.

All of the above models are compatible with Ollama and LiteLLM – e.g. LiteLLM “supports all models from Ollama” ￼. These models have either official or community GGML/GGUF quantizations that Ollama can download, or you can run them via LiteLLM’s OpenAI-compatible API by pointing to a local inference server. They are instruction-tuned to respect system prompts (for example, Qwen and Llama2 models explicitly support a system role in their chat format ￼) and several have been optimized or evaluated for tool usage (Qwen in particular excelling at function-style outputs ￼).

Hardware Upgrade: RTX 3050 → RTX 5070

Upgrading the RTX 3050 to an RTX 5070 will provide a significant boost in LLM inference performance. Key impacts and new capabilities include:
	•	Higher Throughput & New Architecture: The RTX 5070 is built on Nvidia’s new Blackwell architecture (5nm “GB205” GPU) with 6144 CUDA cores and 192 Tensor Cores ￼. It delivers roughly a 20–25% gen-on-gen performance uplift over the RTX 4070 ￼, and far more over a 3050 (which is two generations older and a lower tier). In practice, you can expect much faster token generation speeds and the ability to use higher precision or larger models without the 3050 becoming a bottleneck. The 5070’s tensor cores and improved throughput will accelerate transformer computations (it’s “great for the price” in creative/AI tasks according to reviews ￼).
	•	More VRAM (though still 12GB): The RTX 5070 comes with 12 GB of GDDR7 VRAM ￼. This is an increase from the 8 GB on a typical RTX 3050, meaning you’ll have a bit more room to load larger models or longer context. For example, a 13B parameter model in 4-bit quantization (~6–8 GB) fits comfortably, and even a 30B model (~15 GB in 4-bit) could be split across the 12 GB + 12 GB (with your RTX 3060) plus system RAM. New models or context lengths become feasible – e.g. some 30B–34B models or using the full 16k context on a model without running out of memory. However, note that 12GB is still a limiting factor for the very largest models (70B+ would still require offloading to CPU memory). The upgrade ensures that 16k context windows are easier to handle, as the GPU can hold more of the extended context at once, reducing slowdowns.
	•	Overall Impact: Going from a 3050 to a 5070 is a major jump in raw compute (the 5070 rivals last-gen high-end cards in some metrics ￼). For LLM inference, this means snappier responses and the ability to experiment with larger models or more complex prompt chains. It’s not just benchmark numbers – in practical use you’ll find the 5070 can generate longer outputs without timing out and can sustain higher token-per-second rates. The upgrade is significant for local AI workloads. The only caveat is that the VRAM increase (8→12 GB) while helpful, is not huge – it unlocks some models, but truly massive models still need either quantization or multiple GPUs. In summary, expect a noticeable performance improvement and somewhat expanded capability, but not an unlimited runway on model size.

Integrating Speech-to-Text and Text-to-Speech Locally

Co-locating speech recognition (STT) and speech synthesis (TTS) models alongside your LLM is doable on your machine, and you have enough resources to handle both. Here’s some guidance:

Speech-to-Text (STT)

For local speech recognition, OpenAI’s Whisper model is the gold standard. Whisper’s largest model (Large-v2) is highly accurate for multiple languages and domains. Running Whisper requires substantial VRAM – about 10 GB for the large model during inference ￼. Your RTX 3060 (12GB) can comfortably handle Whisper Large. If you need to conserve GPU memory or improve speed, Whisper’s medium or small models use less memory (medium ~5GB VRAM ￼) at the cost of some accuracy.
	•	Model Recommendation: Whisper Large-v2 for best accuracy ￼. It will transcribe speech in (near) real-time on a modern GPU. Even the slowest GPUs tested could transcribe 700+ words/minute with medium, and Large is roughly half as fast ￼ – meaning on a 3060/5070 you can easily surpass human speaking rates (Whisper runs faster than real-time on these GPUs). If needed, you can drop to Whisper Medium to halve memory usage while still retaining strong accuracy.
	•	Compatibility: Whisper can be integrated into your stack via libraries (OpenAI Whisper, FasterWhisper, etc.) and doesn’t conflict with Ollama/LiteLLM – it’s a separate model. It will benefit from using a dedicated GPU (see below), but it’s also possible to run on CPU if necessary (just much slower, likely too slow for interactive use).
	•	Alternative STT: If for some reason Whisper doesn’t suit, other open STT models include NVIDIA’s NeMo ASR models or Coqui STT. However, those often require similar resources and tend to lag Whisper in accuracy for open-domain speech. Whisper’s robust performance and ease of use make it the top choice.

Text-to-Speech (TTS)

For generating speech from the assistant’s replies, you have a few options depending on the quality vs. speed trade-off:
	•	Suno Bark: Bark is a powerful generative audio model that can produce very natural, expressive speech (and even music or sound effects) from text. It’s multi-lingual and can convey emotion, but it is resource-intensive. The full Bark model requires ~12 GB VRAM to run entirely on GPU ￼. It can run on CPU, but slowly (and may struggle on long outputs). Bark can generate ~13 seconds of audio per pass by default and can be used for longer clips with some work. If you desire highly natural, varied voices or non-English support out-of-the-box, Bark is an option – just plan to allocate a GPU for it. (There is a “small models” mode that fits in 8GB ￼, trading off some quality.) Users have found Bark to be a bit unstable at times (voice quality can drift on very long texts ￼), but for short responses it works well.
	•	Lightweight TTS (Piper / VITS): If you just need a clear, natural-sounding voice and fast response, a lighter TTS system is preferable. Piper (also known as Mimic3) is a fast, local neural TTS that supports multiple pre-trained voices. It uses efficient models (often based on Glow-TTS or FastSpeech2 with HiFi-GAN vocoders) that can run in real-time on CPU. In fact, Piper is designed to be CPU-friendly and doesn’t require a GPU for good performance – “fast, doesn’t need a GPU, and doesn’t destroy the CPU” as users note. It can synthesize speech on an 8-core CPU at or near real-time speed, and if you enable GPU acceleration (ONNX Runtime with CUDA) it will be even faster ￼. Piper comes with many voices (English and other languages) – though these are static voices, not the generative flexibility of Bark. For a personal assistant use-case, Piper (or similar systems like Coqui TTS with a pretrained voice) is often a great choice because it’s efficient and the speech is very intelligible and pleasant for longer listening. The trade-off is that it won’t do things like singing, emotional tone shifts, or arbitrary speaker styles – it will read text in a relatively natural but fixed voice.
	•	Other TTS: There are other open TTS projects – e.g. Tortoise TTS (very high quality, can clone voices, but extremely slow – not practical for real-time), and VITS models released by various orgs (some multi-lingual, some high-speed). Many of these have been integrated into the Piper/Mimic framework or can be run via Coqui’s repository. If you need multiple voices or languages with moderate quality, look into the voices available for Piper or Coqui – many are very good. For a single English voice, a “medium” sized Tacotron2 or FastSpeech2 + HiFiGAN model (tens of millions of params) can generate near real-time on CPU. In sum, unless you specifically need Bark’s advanced capabilities, a lightweight TTS will be easier to run alongside your LLM.

GPU Resource Allocation for LLM + Audio

With dual GPUs (and a strong CPU), you have flexibility in how to allocate tasks:
	•	Use Separate GPUs: The ideal setup is to dedicate one GPU to the LLM and another to the audio pipeline. For instance, you could run your LLM on the new RTX 5070, and run Whisper (STT) and your TTS model on the RTX 3060. This separation is beneficial because both LLM inference and high-quality STT/TTS are heavy on VRAM and compute. Whisper large using ~10GB VRAM ￼ and Bark using up to 12GB ￼ would each nearly saturate a GPU – so trying to run them on the same card as the LLM can cause memory swapping or slowdowns. By using the 3060 for audio, you ensure the LLM (on the 5070) can use its full memory and horsepower. It also allows parallelism – e.g. the LLM can generate the next answer while the other GPU is still finishing TTS for the previous answer, making the system more responsive overall.
	•	Shared GPU (if necessary): It is possible to run everything on one GPU, especially with careful scheduling – for example, generate the text first, then unload the LLM model or pause it while running TTS on the same GPU. LiteLLM/Ollama won’t natively manage that swapping, but you could manually orchestrate loading/unloading models. However, this introduces overhead and latency. Given you have the hardware, splitting the workload is the way to go. The 128 GB of system RAM also means you have plenty of memory to host models when they’re not on GPU (for example, you could keep Whisper on CPU memory and only load it to GPU when needed, if GPU memory ever became tight).
	•	CPU for TTS as an option: As mentioned, a lightweight TTS like Piper can run on the CPU with acceptable performance. Your 8-core Ryzen and large RAM mean the CPU can handle TTS duty without lag for reasonably-sized outputs (especially if using multithreading). This could free the second GPU for Whisper exclusively. In practice, though, the 3060 is likely idle while Whisper isn’t transcribing, so you might as well use it for TTS too. Just be mindful of not loading too many models on the 3060 at once (e.g., Whisper + Bark together might exceed 12GB if both at full size). If using Bark, you might load/unload its models as needed or use the 8GB small mode to fit alongside Whisper.

Bottom line: You can absolutely co-locate the speech models with the LLM. For best results, dedicate the RTX 3060 to running Whisper for STT and whichever TTS model you choose, while the RTX 5070 focuses on the LLM. This setup will comfortably handle a full voice assistant pipeline – the STT will quickly transcribe user speech, feed it to the LLM (on the other GPU) for response generation, and then the TTS will render the reply audio. Each component is running at near real-time speed on its designated hardware, so the end-to-end interaction should feel natural. Shared use on one GPU is technically feasible but not optimal given your resources. Separating the workloads avoids contention and ensures the LLM’s responsiveness isn’t affected by audio processing tasks.

Sources:
	•	Open-source LLM capabilities (context length, tool use, etc.): EverythingLM (Llama2 13B, 16k) ￼; Mistral 7B Instruct (32k context) ￼; Cohere Command models (128k context & tool use) ￼; Qwen-Chat function calling optimization ￼ and system prompt adherence ￼; Tool-use benchmark showing Qwen-14B near GPT-4 level ￼.
	•	RTX 5070 specs and performance: 12GB GDDR7, 192 tensor cores ￼; ~20% better than RTX 4070 (gen-on-gen uplift) ￼; noted strong AI performance for the price ￼.
	•	Whisper STT VRAM needs and speed: ~5GB for medium, ~10GB for large ￼; real-time transcription rates on GPUs ￼.
	•	Bark TTS resource requirements: ~12GB VRAM full model, 8GB in small mode ￼; user experiences with Bark quality ￼.
	•	Piper/Mimic3 TTS efficiency: user report of near-real-time on a GTX 970 (older GPU) ￼, indicating how a GPU can speed it up significantly. (And by extension, modern CPUs/GPUs handle it with ease.)
    
If your primary goal is maximizing VRAM per dollar on an NVIDIA card (in early 2025), the consensus among AI hobbyists and small-scale labs is that a used RTX 3090 or 3090 Ti usually wins:
	1.	24GB of GDDR6X VRAM for sub–$500–$600 on the used market (often less, depending on local availability).
	2.	Enough CUDA/Tensor cores to handle large model inference or GPU training in a single card.
	3.	The 3090 series was widely purchased by crypto miners and enthusiast gamers, creating a healthy resale inventory – so used prices drop faster than new-series cards.

Why the 3090?
	•	VRAM – 24GB is enough for many 13B–30B parameter models in lower-precision quantization without multi-GPU.
	•	Price – Cheaper than a new 4090 (also 24GB) or the upcoming 5070/5080 series.
	•	Throughput – Still strong enough to handle high token-per-second rates on typical open-source models.

Alternatives
	•	4090 (24GB) – Also has 24GB and far greater performance. But the new or lightly used prices remain high (often $1,200–$1,500+). That’s worse VRAM/$ even though it’s faster overall.
	•	4070 (12GB), 4080 (16GB) – Plenty for mainstream gaming, but 12–16GB can be limiting for big LLM inference. And their cost-per-GB ratio is typically worse.
	•	New 5070 (12GB) or 5080 (16GB) – Great performance, but not enough VRAM for big LLMs unless you rely heavily on multi-GPU or CPU offloading. Also new-generation cards typically command a premium.

Hence, if your single priority is “max VRAM per dollar,” a used 3090 is hard to beat in early 2025.